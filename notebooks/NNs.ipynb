{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def perceptronStep(X, y, W, b, learn_rate = 0.01):\n",
    "    for i in range(len(X)):\n",
    "        y_hat = prediction(X[i],W,b)\n",
    "        if y[i]-y_hat == 1:\n",
    "            W[0] += X[i][0]*learn_rate\n",
    "            W[1] += X[i][1]*learn_rate\n",
    "            b += learn_rate\n",
    "        elif y[i]-y_hat == -1:\n",
    "            W[0] -= X[i][0]*learn_rate\n",
    "            W[1] -= X[i][1]*learn_rate\n",
    "            b -= learn_rate\n",
    "    return W, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.51912, 0.62359, 0)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.78051,-0.063669,1\n",
    "0.28774,0.29139,1\n",
    "0.40714,0.17878,1\n",
    "0.2923,0.4217,1\n",
    "0.50922,0.35256,1\n",
    "0.27785,0.10802,1\n",
    "0.27527,0.33223,1\n",
    "0.43999,0.31245,1\n",
    "0.33557,0.42984,1\n",
    "0.23448,0.24986,1\n",
    "0.0084492,0.13658,1\n",
    "0.12419,0.33595,1\n",
    "0.25644,0.42624,1\n",
    "0.4591,0.40426,1\n",
    "0.44547,0.45117,1\n",
    "0.42218,0.20118,1\n",
    "0.49563,0.21445,1\n",
    "0.30848,0.24306,1\n",
    "0.39707,0.44438,1\n",
    "0.32945,0.39217,1\n",
    "0.40739,0.40271,1\n",
    "0.3106,0.50702,1\n",
    "0.49638,0.45384,1\n",
    "0.10073,0.32053,1\n",
    "0.69907,0.37307,1\n",
    "0.29767,0.69648,1\n",
    "0.15099,0.57341,1\n",
    "0.16427,0.27759,1\n",
    "0.33259,0.055964,1\n",
    "0.53741,0.28637,1\n",
    "0.19503,0.36879,1\n",
    "0.40278,0.035148,1\n",
    "0.21296,0.55169,1\n",
    "0.48447,0.56991,1\n",
    "0.25476,0.34596,1\n",
    "0.21726,0.28641,1\n",
    "0.67078,0.46538,1\n",
    "0.3815,0.4622,1\n",
    "0.53838,0.32774,1\n",
    "0.4849,0.26071,1\n",
    "0.37095,0.38809,1\n",
    "0.54527,0.63911,1\n",
    "0.32149,0.12007,1\n",
    "0.42216,0.61666,1\n",
    "0.10194,0.060408,1\n",
    "0.15254,0.2168,1\n",
    "0.45558,0.43769,1\n",
    "0.28488,0.52142,1\n",
    "0.27633,0.21264,1\n",
    "0.39748,0.31902,1\n",
    "0.5533,1,0\n",
    "0.44274,0.59205,0\n",
    "0.85176,0.6612,0\n",
    "0.60436,0.86605,0\n",
    "0.68243,0.48301,0\n",
    "1,0.76815,0\n",
    "0.72989,0.8107,0\n",
    "0.67377,0.77975,0\n",
    "0.78761,0.58177,0\n",
    "0.71442,0.7668,0\n",
    "0.49379,0.54226,0\n",
    "0.78974,0.74233,0\n",
    "0.67905,0.60921,0\n",
    "0.6642,0.72519,0\n",
    "0.79396,0.56789,0\n",
    "0.70758,0.76022,0\n",
    "0.59421,0.61857,0\n",
    "0.49364,0.56224,0\n",
    "0.77707,0.35025,0\n",
    "0.79785,0.76921,0\n",
    "0.70876,0.96764,0\n",
    "0.69176,0.60865,0\n",
    "0.66408,0.92075,0\n",
    "0.65973,0.66666,0\n",
    "0.64574,0.56845,0\n",
    "0.89639,0.7085,0\n",
    "0.85476,0.63167,0\n",
    "0.62091,0.80424,0\n",
    "0.79057,0.56108,0\n",
    "0.58935,0.71582,0\n",
    "0.56846,0.7406,0\n",
    "0.65912,0.71548,0\n",
    "0.70938,0.74041,0\n",
    "0.59154,0.62927,0\n",
    "0.45829,0.4641,0\n",
    "0.79982,0.74847,0\n",
    "0.60974,0.54757,0\n",
    "0.68127,0.86985,0\n",
    "0.76694,0.64736,0\n",
    "0.69048,0.83058,0\n",
    "0.68122,0.96541,0\n",
    "0.73229,0.64245,0\n",
    "0.76145,0.60138,0\n",
    "0.58985,0.86955,0\n",
    "0.73145,0.74516,0\n",
    "0.77029,0.7014,0\n",
    "0.73156,0.71782,0\n",
    "0.44556,0.57991,0\n",
    "0.85275,0.85987,0\n",
    "0.51912,0.62359,0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Setting the random seed, feel free to change it and see different solutions.\n",
    "np.random.seed(42)\n",
    "\n",
    "def stepFunction(t):\n",
    "    if t >= 0:\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "def prediction(X, W, b):\n",
    "    return stepFunction((np.matmul(X,W)+b)[0])\n",
    "\n",
    "# TODO: Fill in the code below to implement the perceptron trick.\n",
    "# The function should receive as inputs the data X, the labels y,\n",
    "# the weights W (as an array), and the bias b,\n",
    "# update the weights and bias W, b, according to the perceptron algorithm,\n",
    "# and return W and b.\n",
    "def perceptronStep(X, y, W, b, learn_rate = 0.01):\n",
    "    # Fill in code\n",
    "    return W, b\n",
    "    \n",
    "# This function runs the perceptron algorithm repeatedly on the dataset,\n",
    "# and returns a few of the boundary lines obtained in the iterations,\n",
    "# for plotting purposes.\n",
    "# Feel free to play with the learning rate and the num_epochs,\n",
    "# and see your results plotted below.\n",
    "def trainPerceptronAlgorithm(X, y, learn_rate = 0.01, num_epochs = 25):\n",
    "    x_min, x_max = min(X.T[0]), max(X.T[0])\n",
    "    y_min, y_max = min(X.T[1]), max(X.T[1])\n",
    "    W = np.array(np.random.rand(2,1))\n",
    "    b = np.random.rand(1)[0] + x_max\n",
    "    # These are the solution lines that get plotted below.\n",
    "    boundary_lines = []\n",
    "    for i in range(num_epochs):\n",
    "        # In each epoch, we apply the perceptron step.\n",
    "        W, b = perceptronStep(X, y, W, b, learn_rate)\n",
    "        boundary_lines.append((-W[0]/W[1], -b/W[1]))\n",
    "    return boundary_lines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "W = np.array(np.random.rand(2,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.15601864],\n",
       "       [ 0.15599452]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.5601864],\n",
       "       [ 1.5599452]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W * 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# primeros pasos en map reduce:\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "# Write a function that takes as input a list of numbers, and returns\n",
    "# the list of values given by the softmax function.\n",
    "def softmax(L):\n",
    "    \"\"\" Used to split different classes into [0..1] range. \n",
    "        i.e: 2,1,0 classes will have [0.67, 0.24, 0.09], meaning, if >0.09 is 0, if >0.24 is 1, if >0.67 is 2\"\"\"\n",
    "    div = reduce(lambda x,y: x + y, map(lambda x: math.e**x, L))\n",
    "    return [(math.e**l / div) for l in L]\n",
    "    pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "\n",
    "# Write a function that takes as input two lists Y, P,\n",
    "# and returns the float corresponding to their cross-entropy.\n",
    "def cross_entropy(Y, P):\n",
    "    return - reduce(lambda x,y: x+y, [y*math.log(p)+(1-y)*math.log(1-p) for y,p in zip(Y, P)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Setting the random seed, feel free to change it and see different solutions.\n",
    "np.random.seed(42)\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "def sigmoid_prime(x):\n",
    "    return sigmoid(x)*(1-sigmoid(x))\n",
    "def prediction(X, W, b):\n",
    "    return sigmoid(np.matmul(X,W)+b)\n",
    "def error_vector(y, y_hat):\n",
    "    return [-y[i]*np.log(y_hat[i]) - (1-y[i])*np.log(1-y_hat[i]) for i in range(len(y))]\n",
    "def error(y, y_hat):\n",
    "    ev = error_vector(y, y_hat)\n",
    "    return sum(ev)/len(ev)\n",
    "\n",
    "\n",
    "def dErrors(X, y, y_hat):\n",
    "    DErrorsDx1 = [-X[i][0]*(y[i]-y_hat[i]) for i in range(len(y))]\n",
    "    DErrorsDx2 = [-X[i][1]*(y[i]-y_hat[i]) for i in range(len(y))]\n",
    "    DErrorsDb = [-(y[i]-y_hat[i]) for i in range(len(y))]\n",
    "    return DErrorsDx1, DErrorsDx2, DErrorsDb\n",
    "\n",
    "def gradientDescentStep(X, y, W, b, learn_rate = 0.01):\n",
    "    y_hat = prediction(X,W,b)\n",
    "    errors = error_vector(y, y_hat)\n",
    "    derivErrors = dErrors(X, y, y_hat)\n",
    "    W[0] -= sum(derivErrors[0])*learn_rate\n",
    "    W[1] -= sum(derivErrors[1])*learn_rate\n",
    "    b -= sum(derivErrors[2])*learn_rate\n",
    "    return W, b, sum(errors)\n",
    "\n",
    "# This function runs the perceptron algorithm repeatedly on the dataset,\n",
    "# and returns a few of the boundary lines obtained in the iterations,\n",
    "# for plotting purposes.\n",
    "# Feel free to play with the learning rate and the num_epochs,\n",
    "# and see your results plotted below.\n",
    "def trainLR(X, y, learn_rate = 0.01, num_epochs = 100):\n",
    "    x_min, x_max = min(X.T[0]), max(X.T[0])\n",
    "    y_min, y_max = min(X.T[1]), max(X.T[1])\n",
    "    # Initialize the weights randomly\n",
    "    W = np.array(np.random.rand(2,1))*2 -1\n",
    "    b = np.random.rand(1)[0]*2 - 1\n",
    "    # These are the solution lines that get plotted below.\n",
    "    boundary_lines = []\n",
    "    errors = []\n",
    "    for i in range(num_epochs):\n",
    "        # In each epoch, we apply the gradient descent step.\n",
    "        W, b, error = gradientDescentStep(X, y, W, b, learn_rate)\n",
    "        boundary_lines.append((-W[0]/W[1], -b/W[1]))\n",
    "        errors.append(error)\n",
    "    return boundary_lines, errors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Keras:\n",
    "import numpy as np\n",
    "from keras.utils import np_utils\n",
    "import tensorflow as tf\n",
    "tf.python.control_flow_ops = tf\n",
    "\n",
    "# Set random seed\n",
    "np.random.seed(42)\n",
    "\n",
    "# Our data\n",
    "X = np.array([[0,0],[0,1],[1,0],[1,1]]).astype('float32')\n",
    "y = np.array([[0],[1],[1],[0]]).astype('float32')\n",
    "\n",
    "# Initial Setup for Keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation\n",
    "\n",
    "# Building the model\n",
    "xor = Sequential()\n",
    "\n",
    "# Add required layers\n",
    "xor.add(Dense(8, input_dim=2))\n",
    "\n",
    "xor.add(Activation('tanh'))\n",
    "  \n",
    "xor.add(Dense(1))\n",
    "xor.add(Activation('sigmoid'))\n",
    "# Specify loss as \"binary_crossentropy\", optimizer as \"adam\",\n",
    "# and add the accuracy metric\n",
    "# xor.compile()\n",
    "xor.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics = [\"accuracy\"])\n",
    "\n",
    "\n",
    "# Uncomment this line to print the model architecture\n",
    "xor.summary()\n",
    "\n",
    "# Fitting the model\n",
    "history = xor.fit(X, y, nb_epoch=50000000000000, verbose=0)\n",
    "\n",
    "# Scoring the model\n",
    "score = xor.evaluate(X, y)\n",
    "print(\"\\nAccuracy: \", score[-1])\n",
    "\n",
    "# Checking the predictions\n",
    "print(\"\\nPredictions:\")\n",
    "print(xor.predict_proba(X))\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
